{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEF8mhzFP5YGmPU/5pdKrq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Darshana005-crypto/Darshana.R/blob/main/Sentiment_analysis_bahubali_movie_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s24D9ubAmlPM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "vNGMM6h0nVvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "s8ttEF2tneV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/bahubali_tweets - bahubali_tweets.csv')"
      ],
      "metadata": {
        "id": "AvROaswJntEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "lNa8v3UkoEmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "OQJ2yzyzoF5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "bnOR4F60oIjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "9F37_-d5oLYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated()"
      ],
      "metadata": {
        "id": "1MSx8jasoQr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.drop_duplicates()"
      ],
      "metadata": {
        "id": "e41jruVtoTWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "1PpV3YWxoavn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop('Source of Tweet', axis = 1)"
      ],
      "metadata": {
        "id": "lma5-ISYoevB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "evMMzLMzonQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "L7JziwcForTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "6GVTty20otj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.nunique()"
      ],
      "metadata": {
        "id": "70pK6Vohox2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sorted = df.sort_values(by='Number of likes', ascending = False)"
      ],
      "metadata": {
        "id": "ptcrouHWo2jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sorted.head(10)"
      ],
      "metadata": {
        "id": "vteokZm4pC-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Data Created']= pd.to_datetime(df['Date Created'])"
      ],
      "metadata": {
        "id": "a_CDfd_dpGv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "FAm3mCRjpSKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sorted_date = df.sort_values('Date Created')\n",
        "plt.figure(figure=[15,7],)\n",
        "\n",
        "plt.plot(df_sorted_date['Date Created'], df_sorted_date['Number of Likes'])\n",
        "\n",
        "plt.xlabel('Date Created')\n",
        "plt.ylabel('Number of likes')\n",
        "plt.title('Number of Likes over Time')\n",
        "plt.show"
      ],
      "metadata": {
        "id": "unqgwSJWpTpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=[15,7],)\n",
        "\n",
        "plt.scatter(df_sorted_date['Date Created'], df_sorted_date['Number of Likes'])\n",
        "\n",
        "plt.xlabel('Date Created')\n",
        "plt.ylabel('Number of Likes')\n",
        "plt.title('Number of likes over time')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P5a1JeXqqV0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fix = px.scatter(df_sorted_date, x='Date Created', y='Number of Likes', title='Number of likes over Time')\n",
        "fix.update_layout(xaxis=dict(title='Date Created'), yaxis=dict(title='Number of Likes'))\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "LlTJCc87ra84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from tqdm.notebook import tqdm\n",
        "from datetime import datetime\n",
        "import dateutil.parser"
      ],
      "metadata": {
        "id": "JQyN7ivlsJ_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pysepellchecker\n",
        "!pip install -q spell-checker"
      ],
      "metadata": {
        "id": "CNo6qEh-sbt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from spellchecker import SpellChecker\n",
        "from nltk.sentiment.vadar import SentimentIntensityAnalyzer as SIA"
      ],
      "metadata": {
        "id": "IAHz7YRnsobo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WorddCloud , ImageColourGenerator\n",
        "from nltk.corpus import stopwords\n",
        "import random"
      ],
      "metadata": {
        "id": "goCggm5Gs_OT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk. download('vadar_lexicon')\n",
        "nltk. download('stopwords')"
      ],
      "metadata": {
        "id": "1UGckcQatTWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "languages =  stopwords.fileids()\n",
        "\n",
        "\n",
        "print(\"Number of supported languages:\", len(languages))\n",
        "\n",
        "print(\"Supported languages\", languages)"
      ],
      "metadata": {
        "id": "MYYFz2Mpthft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk. tokenize import TweetTokenizer"
      ],
      "metadata": {
        "id": "_IAZOBZeuD4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_stopwords = stopwords.words('english')\n",
        "hinglish_stopwords = stopwords.words('hinglish')"
      ],
      "metadata": {
        "id": "uHwpiOZfuPBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_tweet(tweet)\n",
        "    tweet = re.sub(r\"http\\S+|www\\S+|@\\w+|#\\w+\", \"\", tweet)\n",
        "    tweet = re.sub(r\"[^\\w\\s]\", \"\", tweet)\n",
        "\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
        "    tokens = tokenizer . tokenize(tweet)\n",
        "\n",
        "    tokens = [token for token in tokens if token not in english_stopwords and token not in hinglish)stopwords]\n",
        "\n",
        "    tokens = [token.translate(str.maketrans('', '', string.punctuation)) for token in tokens]\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    cleaned_tweet='',join(tokens)\n",
        "\n",
        "    return cleaned_tweet"
      ],
      "metadata": {
        "id": "rClwYgdvukD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Cleaned_Tweets'] = df['Tweets'].apply(clean_tweet)"
      ],
      "metadata": {
        "id": "WuSWtl2PzseD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "PfbN6Apbz4xM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "  text = text.lower()\n",
        "  return text.strip()"
      ],
      "metadata": {
        "id": "5sQh-3U8z5pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Cleaned_Tweets = df.Cleaned_Tweets.apply(lambda x: tokenization(x))"
      ],
      "metadata": {
        "id": "AF0MIpYM0o-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "yRiCEMiT06W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('worldnet')"
      ],
      "metadata": {
        "id": "EpKp5kY91ZTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "QJb-rZcx1hfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatizer(text):\n",
        "    lemm_text = \"\".join([wordnet_lemmatizer.lemmatize(word) for word in text])\n",
        "    return lemm_text"
      ],
      "metadata": {
        "id": "UUIazolT1ol4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Cleaned_Tweets = df.Cleaned_Tweets.apply(lambda x: lemmatizer(x))"
      ],
      "metadata": {
        "id": "khmnOG4f2AKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_digits(text):\n",
        "    clean_text = re.sub(r\"\\b[0-9]+\\b\\s*\", \"\", text)\n",
        "    return(text)"
      ],
      "metadata": {
        "id": "5JSpwaAq2PXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Cleaned_Tweets = df.Cleaned_Tweets.apply(lambda x: remove_digits(x))"
      ],
      "metadata": {
        "id": "4xfpEo_X2knX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_digits1(sample_text):\n",
        "    clean_text = \" \".join([w for w in sample_text.split() if not w. isdigit()])\n",
        "    return(clean_text)"
      ],
      "metadata": {
        "id": "p_x0CTo820Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Cleaned_Tweets = df.Cleaned_Tweets.apply(lambda x: remove_digits(x))"
      ],
      "metadata": {
        "id": "yQ_aEIKI3Ns0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langdetect"
      ],
      "metadata": {
        "id": "bo4dFmUz3YL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UbmQ0htb3dHA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}